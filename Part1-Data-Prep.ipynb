{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some context around the project please look the read_me file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first notebook we will generate the data that we will use to fine tune the Llama model. In this case, I will grab the top 1500 posts (all-time) from the askculinary subreddit. Before anything we will import the notebooks we need. Additionally, you will need to create a reddit user agent to scrap data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id='insert-client-id',\n",
    "    client_secret='insert-client-secret',\n",
    "    user_agent='insert-agent-name'  \n",
    ")\n",
    "\n",
    "subreddit_name = 'AskCulinary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_posts = reddit.subreddit(subreddit_name).top(time_filter='all', limit=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now grab the posts and the top comments from the posts. In doing so, we will append into the lists which we will be using to input into a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_list = []\n",
    "comments_list = []\n",
    "\n",
    "for post in top_posts:\n",
    "    #print(f\"Title: {post.title}\")\n",
    "    #print(f\"URL: {post.url}\")\n",
    "    titles_list.append(post.title)\n",
    "    post.comments.replace_more(limit=0)\n",
    "    if post.comments:\n",
    "        # Sort comments by score and get the top comment\n",
    "        top_comment = sorted(post.comments, key=lambda comment: comment.score, reverse=True)[0]\n",
    "        #print(f\"Top Comment: {top_comment.body}\\n\")\n",
    "        comments_list.append(top_comment.body)\n",
    "    else:\n",
    "        comments_list.append('No comments found')\n",
    "        #print(\"No comments found\\n\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data = {'post' : titles_list, 'comments' : comments_list})\n",
    "\n",
    "df.to_csv('data.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No I put my data into a csv for easier data modification. Our next step will be structure our posts comments pairs to follow what our llama model expects. In this case we will insert the [INST] tokens which will indicate were the questions are being placed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will insert the data into the json file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df.apply(lambda row: '<s> [INST] ' + row['post'] + ' [/INST] ' + row['comments'], axis=1)\n",
    "\n",
    "json_lines = df['text'].apply(lambda x: {'text': x}).to_json(orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "json_lines = json_lines.replace('\\\\/', '/')\n",
    "\n",
    "with open('recipe_data.jsonl', 'w', encoding='utf-8') as f:\n",
    "    f.write(json_lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now done with the data generation step of the project. If the model was hosted in a production environment, then we can run this script in a weekly basis were we can input more recent posts from the week. In this way the model will continue to build knowledge. For this proof of concept however, I will just use this initial data for the pretraining step. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
